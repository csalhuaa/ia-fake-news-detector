{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b630a83a",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "Limpieza, tokenizaciÃ³n y vectorizaciÃ³n de los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca77f2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cargado: 1072 filas\n",
      "DistribuciÃ³n: {'fake': 536, 'real': 536}\n",
      "ðŸ”„ Preprocesando textos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1072/1072 [01:23<00:00, 12.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Vectorizando con TF-IDF...\n",
      "Matriz TF-IDF: (1072, 3000)\n",
      "Vocabulario: 3000 palabras\n",
      "Train: 857 | Test: 215\n",
      "âœ… Preprocesamiento completado!\n",
      "ï¿½ï¿½ Archivos guardados:\n",
      "  - /data/split/train.pkl\n",
      "  - /data/split/test.pkl\n",
      "  - /models/vectorizer.pkl\n",
      "  - /data/processed/dataset_procesado.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Descargar recursos NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Cargar modelo de spaCy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# --- 1. Cargar dataset unificado ---\n",
    "df = pd.read_csv('../data/processed/dataset_unificado.csv')\n",
    "print(f\"Dataset cargado: {len(df)} filas\")\n",
    "print(f\"DistribuciÃ³n: {df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# --- 2. Preprocesamiento avanzado ---\n",
    "def preprocesar_texto_avanzado(texto):\n",
    "    if pd.isna(texto) or texto == \"\":\n",
    "        return \"\"\n",
    "    # Normalizar y limpiar\n",
    "    texto = str(texto).lower()\n",
    "    texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto, flags=re.MULTILINE)\n",
    "    texto = re.sub(r'[^a-zÃ¡Ã©Ã­Ã³ÃºÃ±Ã¼\\s]', ' ', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    # Procesar con spaCy\n",
    "    doc = nlp(texto)\n",
    "    # Lematizar, eliminar stopwords y tokens cortos\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.is_alpha and not token.is_stop and len(token.lemma_) > 2\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# --- 3. Aplicar preprocesamiento ---\n",
    "print(\"ðŸ”„ Preprocesando textos...\")\n",
    "df['texto_procesado'] = df['contenido'].progress_apply(preprocesar_texto_avanzado)\n",
    "\n",
    "# --- 4. VectorizaciÃ³n TF-IDF optimizada ---\n",
    "print(\"ðŸ”„ Vectorizando con TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,  # Reducido para evitar overfitting\n",
    "    min_df=2,          # Palabra debe aparecer en al menos 2 documentos\n",
    "    max_df=0.95,       # Palabra no debe aparecer en mÃ¡s del 95% de documentos\n",
    "    ngram_range=(1, 2), # Unigramas y bigramas\n",
    "    stop_words=None    # Ya eliminamos stopwords\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df['texto_procesado'])\n",
    "y = df['label'].map({'fake': 0, 'real': 1})\n",
    "\n",
    "print(f\"Matriz TF-IDF: {X.shape}\")\n",
    "print(f\"Vocabulario: {len(vectorizer.vocabulary_)} palabras\")\n",
    "\n",
    "# --- 5. Split estratificado ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "\n",
    "# --- 6. Guardar archivos ---\n",
    "os.makedirs('../data/split', exist_ok=True)\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Guardar datos procesados\n",
    "joblib.dump((X_train, y_train), '../data/split/train.pkl')\n",
    "joblib.dump((X_test, y_test), '../data/split/test.pkl')\n",
    "\n",
    "# Guardar vectorizador\n",
    "joblib.dump(vectorizer, '../models/vectorizer.pkl')\n",
    "\n",
    "# Guardar dataset procesado\n",
    "df_procesado = df[['contenido', 'texto_procesado', 'label']]\n",
    "df_procesado.to_csv('../data/processed/dataset_procesado.csv', index=False)\n",
    "\n",
    "print(\"âœ… Preprocesamiento completado!\")\n",
    "print(\"ï¿½ï¿½ Archivos guardados:\")\n",
    "print(\"  - /data/split/train.pkl\")\n",
    "print(\"  - /data/split/test.pkl\") \n",
    "print(\"  - /models/vectorizer.pkl\")\n",
    "print(\"  - /data/processed/dataset_procesado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f16f1a-aafb-4641-9b6f-f05e17e12fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
