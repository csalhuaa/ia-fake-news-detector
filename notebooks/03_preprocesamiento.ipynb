{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b630a83a",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos\n",
    "Limpieza, tokenizaci√≥n y vectorizaci√≥n de los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77f2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Columnas: ['ID', 'Label', 'Titulo', 'Descripcion', 'Fecha']\n",
      "‚úÖ Preprocesamiento y vectorizaci√≥n completados.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Rutas\n",
    "raw_path       = os.path.join(\"..\", \"data\", \"raw\", \"D57000_complete.csv\")\n",
    "processed_dir  = os.path.join(\"..\", \"data\", \"processed\")\n",
    "split_dir      = os.path.join(\"..\", \"data\", \"split\")\n",
    "vectorizer_dir = os.path.join(\"..\", \"models\")\n",
    "vectorizer_path = os.path.join(vectorizer_dir, \"vectorizer.pkl\")\n",
    "\n",
    "# Leer archivo con separador adecuado\n",
    "df = pd.read_csv(raw_path, sep=\";\", encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "# Verificamos columnas: ['ID', 'Label', 'Titulo', 'Descripcion', 'Fecha']\n",
    "print(\"üîπ Columnas:\", df.columns.tolist())\n",
    "\n",
    "# Combinamos Titulo y Descripcion como texto\n",
    "df[\"text\"] = df[\"Titulo\"].fillna(\"\") + \". \" + df[\"Descripcion\"].fillna(\"\")\n",
    "\n",
    "# Mantenemos solo texto y etiqueta\n",
    "df = df[[\"text\", \"Label\"]].rename(columns={\"Label\": \"label\"})\n",
    "\n",
    "# Limpieza del texto\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r\"http\\S+\", \"\", texto)  # quitar URLs\n",
    "    texto = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√º\\s]\", \"\", texto)  # solo letras y espacios\n",
    "    texto = re.sub(r\"\\s+\", \" \", texto).strip()\n",
    "    return texto\n",
    "\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\").apply(limpiar_texto)\n",
    "\n",
    "# Eliminar stopwords en espa√±ol\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "\n",
    "def eliminar_stopwords(texto):\n",
    "    return \" \".join([palabra for palabra in texto.split() if palabra not in stopwords_es])\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(eliminar_stopwords)\n",
    "\n",
    "# Guardar versi√≥n preprocesada\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "df.to_csv(os.path.join(processed_dir, \"datos_preprocesados.csv\"), index=False)\n",
    "\n",
    "# Vectorizaci√≥n con TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df[\"text\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Guardar vectorizador\n",
    "os.makedirs(vectorizer_dir, exist_ok=True)\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "\n",
    "# Separar en entrenamiento y prueba (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Guardar como archivos serializados\n",
    "os.makedirs(split_dir, exist_ok=True)\n",
    "joblib.dump((X_train, y_train), os.path.join(split_dir, \"train.pkl\"))\n",
    "joblib.dump((X_test, y_test), os.path.join(split_dir, \"test.pkl\"))\n",
    "\n",
    "print(\"‚úÖ Preprocesamiento y vectorizaci√≥n completados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f16f1a-aafb-4641-9b6f-f05e17e12fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
